{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLoGu9BuC-P5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Deep Image Prior Approach"
      ],
      "metadata": {
        "id": "PMOIhioGDCsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper Fns"
      ],
      "metadata": {
        "id": "dMv6Z8ZWDIAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function to load image and convert to tensor. This function will\n",
        "rescale the pixel values to lie in [0,1], and add a batch dimension so that\n",
        "the image shape is (batch, channels, height, width)\n",
        "\"\"\"\n",
        "def load_image_as_tensor(img_path, img_size=256):\n",
        "  img = Image.open(img_path)\n",
        "  transform = transforms.Compose([transforms.Resize(img_size), transforms.ToTensor()])\n",
        "  return transform(img).unsqueeze(0)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Function to convert a tensor image into a numpy image. Input image shape is assumed\n",
        "to be (batch, channels, height, width). Output image shape will be (height, width, channels)\n",
        "\"\"\"\n",
        "def tensor_image_to_numpy(img_tensor):\n",
        "  img_np = img_tensor.detach().cpu().numpy()\n",
        "  return np.transpose( img_np[0,...], (1, 2, 0))\n",
        "\n",
        "\"\"\"\n",
        "Function to add per-pixel gaussian noise to a given image, with standard deviation of sigma.\n",
        "\"\"\"\n",
        "def add_noise_to_image(x, sigma):\n",
        "  x_noise = x + torch.normal(torch.zeros(x.shape), torch.ones(x.shape)*sigma)\n",
        "  return x_noise.to(x.device)"
      ],
      "metadata": {
        "id": "Wz1IZdAaDCUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function to load image and convert to tensor. This function will\n",
        "rescale the pixel values to lie in [0,1], and add a batch dimension so that\n",
        "the image shape is (batch, channels, height, width)\n",
        "\"\"\"\n",
        "def load_image_as_tensor(img_path, img_size=512):\n",
        "  img = Image.open(img_path)\n",
        "  transform = transforms.Compose([transforms.Resize(img_size), transforms.ToTensor()])\n",
        "  return transform(img).unsqueeze(0)\n",
        "\n",
        "\"\"\"\n",
        "Function to convert a tensor image into a numpy image. Input image shape is assumed\n",
        "to be (batch, channels, height, width). Output image shape will be (height, width, channels)\n",
        "\"\"\"\n",
        "def tensor_image_to_numpy(img_tensor):\n",
        "  img_np = img_tensor.detach().cpu().numpy()\n",
        "  return np.transpose( img_np[0,...], (1, 2, 0))\n",
        "\n",
        "\"\"\"\n",
        "Function to add per-pixel gaussian noise to a given image, with standard deviation of sigma.\n",
        "\"\"\"\n",
        "def add_noise_to_image(x, sigma):\n",
        "  x_noise = x + torch.normal(torch.zeros(x.shape), torch.ones(x.shape)*sigma)\n",
        "  return x_noise.to(x.device)\n",
        "\n",
        "\"\"\"\n",
        "Function to display a grid of images.\n",
        "\"\"\"\n",
        "def image_grid(imgs, rows, cols):\n",
        "  assert len(imgs) == rows*cols\n",
        "\n",
        "  w, h = imgs[0].size\n",
        "  grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "  grid_w, grid_h = grid.size\n",
        "\n",
        "  for i, img in enumerate(imgs):\n",
        "      grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "  return grid"
      ],
      "metadata": {
        "id": "NyM78Mo6D-xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "U-Net"
      ],
      "metadata": {
        "id": "lBY52hyZDLui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class encoder_block(nn.Module):\n",
        "  def __init__(self, in_channels, conv_channels):\n",
        "    super(encoder_block, self).__init__()\n",
        "\n",
        "    self.nn_layers = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, conv_channels, kernel_size=3, padding='same'),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.BatchNorm2d(conv_channels),\n",
        "        nn.Conv2d(conv_channels, conv_channels, kernel_size=3, stride=2, padding=1),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.BatchNorm2d(conv_channels),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.nn_layers(x)"
      ],
      "metadata": {
        "id": "FbAlnfoWDJ2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class decoder_block(nn.Module):\n",
        "  def __init__(self, conv_channels, out_channels):\n",
        "    super(decoder_block, self).__init__()\n",
        "\n",
        "    self.nn_layers = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(conv_channels, conv_channels, kernel_size=3, padding='same'),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.BatchNorm2d(conv_channels),\n",
        "        nn.Conv2d(conv_channels, out_channels, kernel_size=3, padding='same'),\n",
        "        nn.Sigmoid(),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.nn_layers(x)"
      ],
      "metadata": {
        "id": "s4Xos_SYDNJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class end_block(nn.Module):\n",
        "  def __init__(self, conv_channels, out_channels):\n",
        "    super(end_block, self).__init__()\n",
        "\n",
        "    self.nn_layers = nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2),\n",
        "        nn.Conv2d(conv_channels, conv_channels, kernel_size=3, padding='same'),\n",
        "        nn.BatchNorm2d(conv_channels),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Conv2d(conv_channels, out_channels, kernel_size=3, padding='same')\n",
        "        #nn.BatchNorm2d(out_channels),\n",
        "        #nn.LeakyReLU(),\n",
        "\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.nn_layers(x)"
      ],
      "metadata": {
        "id": "cb_nkGjEDQYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lass ENCODER_DECODER(nn.Module):\n",
        "    def __init__(self, depth = 4, in_channels = 3, out_channels = 3, conv_channels = 128):\n",
        "        super(ENCODER_DECODER, self).__init__()\n",
        "\n",
        "        self.depth = depth\n",
        "        self.in_channels = in_channels # Number of channels of input image\n",
        "        self.out_channels = out_channels # Number of channels of output image\n",
        "        self.conv_channels = conv_channels # Number of output channels per convolution\n",
        "\n",
        "        self.block_array = nn.ModuleList()\n",
        "\n",
        "\n",
        "        self.block_array.append(encoder_block(in_channels, conv_channels))\n",
        "        self.block_array.append(encoder_block(conv_channels, conv_channels*2))\n",
        "        self.block_array.append(encoder_block(conv_channels*2, conv_channels*4))\n",
        "        self.block_array.append(encoder_block(conv_channels*4, conv_channels*8))\n",
        "        self.block_array.append(decoder_block(conv_channels*8, conv_channels*4))\n",
        "        self.block_array.append(decoder_block(conv_channels*4, conv_channels*2))\n",
        "        self.block_array.append(decoder_block(conv_channels*2, conv_channels))\n",
        "        self.block_array.append(end_block(conv_channels, out_channels))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.block_array:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "a4wwHWD0DSiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actual Masking and Inpainting:"
      ],
      "metadata": {
        "id": "1ezOVKjHDXK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This function will take an input image tensor of shape (batch, channels, height, width),\n",
        "and set all pixel values to 0 within the n x n region with top left corner located at x,y.\n",
        "\"\"\"\n",
        "def black_out_region(img, n, x, y):\n",
        "  mask = torch.ones((img.shape))\n",
        "  mask[:, :, x:x+n, y:y+n] = 0\n",
        "  return mask\n"
      ],
      "metadata": {
        "id": "ZDw92-ftDW2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, load some real image\n",
        "img_path = 'endless_field.jpg' # REPLACE WITH YOUR IMAGE NAME HERE\n",
        "img_size = 256\n",
        "x = load_image_as_tensor(img_path, img_size).to('cuda')\n",
        "\n",
        "# inpainted\n",
        "mask = black_out_region(x, 32, 150, 150).to('cuda')\n",
        "x_inpainted = mask * x\n",
        "\n",
        "# If you want to show your image, you can do something like this:\n",
        "x_inpainted_np = tensor_image_to_numpy(x_inpainted)\n",
        "plt.imshow(x_inpainted_np)\n",
        "plt.axis(False)\n",
        "print(x_inpainted_np.min())\n",
        "print(x_inpainted_np.max())"
      ],
      "metadata": {
        "id": "ro_UAKqsDeO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, run gradient descent using your CNNs to denoise your image! Use the ADAM optimizer\n",
        "# with a learning rate <= 0.01.\n",
        "\n",
        "# Create input noise image\n",
        "in_channels = 16 # Number of channels of input noise image\n",
        "input_noise = torch.randn(1,in_channels,img_size,img_size)*0.1\n",
        "input_noise = input_noise.to('cuda')\n",
        "\n",
        "net = ENCODER_DECODER(depth=4, in_channels=in_channels, out_channels=3, conv_channels=256)\n",
        "net = net.to('cuda')\n",
        "lr = 1e-4\n",
        "optim = torch.optim.Adam(net.parameters(), lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim)\n",
        "\n",
        "gt_inpainted_image = x_inpainted.to('cuda')\n",
        "\n",
        "n_iterations = 1000 # Replace with whatever number you want\n",
        "for i in range(n_iterations):\n",
        "\n",
        "  output = net(input_noise)\n",
        "  # loss = torch.sum(torch.square((black_out_region(gt_inpainted_image - output, 32, 150, 150)))) / ((img_size**2)*3)\n",
        "  loss = torch.nn.functional.mse_loss(gt_inpainted_image * mask, output * mask)\n",
        "\n",
        "  # Keep these three following lines as is.\n",
        "  optim.zero_grad()\n",
        "  loss.backward()\n",
        "  optim.step()\n",
        "  scheduler.step(loss)\n",
        "\n",
        "  if i%10 == 0:\n",
        "    print(loss.item())\n",
        "    print(scheduler.get_last_lr())\n",
        "    plt.figure()\n",
        "    plt.imshow(output.permute((0,2,3,1)).detach().cpu().numpy()[0, :, :, :])\n",
        "    plt.show()\n",
        "  if i == 200:\n",
        "    break\n",
        "  # You probably want to write some code here that will store the current image\n",
        "  # into an array sporadically, so that you can create a figure from them.\n",
        "\n"
      ],
      "metadata": {
        "id": "nbAPR4afDtDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approach 2: Stable Diffusion"
      ],
      "metadata": {
        "id": "MzBtVri-DzqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "9oU11pu5DwO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers\n",
        "!pip install transformers scipy ftfy accelerate"
      ],
      "metadata": {
        "id": "Y_r2WGPcD76h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load diffusion model for image generation\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(\"stable-diffusion-v1-5/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "C66yGc8XED-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load diffusion model for image inpainting\n",
        "\n",
        "from diffusers import StableDiffusionInpaintPipeline\n",
        "\n",
        "model_id = \"stabilityai/stable-diffusion-2-inpainting\"\n",
        "pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "jQjcSziAEG2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "!wget -q -nc https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
        "CHECKPOINT_PATH='/content/sam_vit_b_01ec64.pth'\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MODEL_TYPE = \"vit_b\"\n",
        "\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH)\n",
        "sam.to(device=DEVICE)"
      ],
      "metadata": {
        "id": "CdymHBIJELJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Give the path of your image\n",
        "IMAGE_PATH = 'swan.jpeg'\n",
        "# Read the image from the path\n",
        "image = cv2.imread(IMAGE_PATH)\n",
        "print(image)\n",
        "# Convert to RGB format\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH)\n",
        "sam.to(device=DEVICE)\n",
        "mask_predictor = SamPredictor(sam)\n",
        "mask_predictor.set_image(image_rgb)\n",
        "\n",
        "# Provide points as input prompt [X, Y]-coordinates\n",
        "input_point = np.array([[71, 41], [64, 44], [85, 109], [141, 110]])\n",
        "input_label = np.array([1, 1, 1, 1])\n",
        "\n",
        "# Predicting Segmentation mask\n",
        "masks, scores, logits = mask_predictor.predict(\n",
        "    point_coords=input_point,\n",
        "    point_labels=input_label,\n",
        "    multimask_output=False,\n",
        ")\n",
        "\n",
        "\n",
        "mask = masks.astype(float) * 255\n",
        "mask = np.transpose(mask, (1, 2, 0))\n",
        "_ , bw_image = cv2.threshold(mask, 100, 255, cv2.THRESH_BINARY)\n",
        "cv2.imwrite('mask.png', bw_image)\n",
        "del sam, mask_predictor   # delete models to conserve GPU memory\n",
        "\n",
        "image = Image.open(IMAGE_PATH)\n",
        "mask = Image.open('mask.png')\n",
        "prompt = \"a surfer riding a surf board\"\n",
        "output = \"/*.png\"             # output filename\n",
        "\n",
        "# inpainted = inpaint_stablediffusion(image, mask, prompt)\n",
        "inpainted = pipe(image=image, mask_image=mask, prompt=prompt, guidance_scale=2)\n",
        "inpainted['images'][0].save('inpainted.png')\n",
        "inpainted['images'][0]"
      ],
      "metadata": {
        "id": "x-QWL2rQEO1W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}